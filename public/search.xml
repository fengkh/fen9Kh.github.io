<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>2023-07锻炼日志</title>
      <link href="/posts/617f83eb.html"/>
      <url>/posts/617f83eb.html</url>
      
        <content type="html"><![CDATA[<hr><h4 id="2023-07-02">2023-07-02</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推、哑铃上斜卧推</p><hr><h4 id="2023-07-03">2023-07-03</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉</p><hr><h4 id="2023-07-04">2023-07-04</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、坐姿推肩、哑铃飞鸟</p><hr><h4 id="2023-07-05">2023-07-05</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲</p><hr><h4 id="2023-07-07">2023-07-07</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推、哑铃上斜卧推</p><hr><h4 id="2023-07-08">2023-07-08</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉</p><hr><h4 id="2023-07-09">2023-07-09</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、坐姿推肩、哑铃飞鸟</p><hr><h4 id="2023-07-10">2023-07-10</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲</p><hr><h4 id="2023-07-12">2023-07-12</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推、哑铃上斜卧推</p><hr><h4 id="2023-07-13">2023-07-13</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉</p><hr><h4 id="2023-07-14">2023-07-14</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、坐姿推肩、哑铃飞鸟</p><hr><h4 id="2023-07-15">2023-07-15</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲</p><hr><h4 id="2023-07-17">2023-07-17</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推、哑铃上斜卧推</p><hr><h4 id="2023-07-18">2023-07-18</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉</p><hr><h4 id="2023-07-19">2023-07-19</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、坐姿推肩、哑铃飞鸟</p><hr><h4 id="2023-07-20">2023-07-20</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲</p><hr>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
            <tag> Health </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-06锻炼日志</title>
      <link href="/posts/bce95a6e.html"/>
      <url>/posts/bce95a6e.html</url>
      
        <content type="html"><![CDATA[<hr><h4 id="2023-06-01">2023-06-01</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 60~100kg * 5</p><hr><h4 id="2023-06-03">2023-06-03</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推 30~60kg * 5</p><hr><h4 id="2023-06-04">2023-06-04</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、硬拉 30~100kg * 5</p><hr><h4 id="2023-06-05">2023-06-05</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推 20~40kg * 5</p><hr><h4 id="2023-06-06（缺席）">2023-06-06（缺席）</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 20~100kg * 5</p><hr><h4 id="2023-06-08">2023-06-08</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推 30~60kg * 5</p><hr><h4 id="2023-06-09">2023-06-09</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、罗马尼亚硬拉 30~100kg * 5</p><hr><h4 id="2023-06-10">2023-06-10</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、面拉 20~40kg * 5</p><hr><h4 id="2023-06-11">2023-06-11</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 20~100kg * 5</p><hr><h4 id="2023-06-12">2023-06-12</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推 30~60kg * 5</p><hr><h4 id="2023-06-13">2023-06-13</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉 30~100kg * 5</p><hr><h4 id="2023-06-14">2023-06-14</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、面拉 20~40kg * 5</p><hr><h4 id="2023-06-15">2023-06-15</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 20~100kg * 5</p><hr><h4 id="2023-06-16">2023-06-16</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推 30~60kg * 5</p><hr><h4 id="2023-06-19">2023-06-19</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉 30~100kg * 5</p><hr><h4 id="2023-06-20">2023-06-20</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、面拉 20~40kg * 5</p><hr><h4 id="2023-06-21">2023-06-21</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 20~100kg * 5</p><hr><h4 id="2023-06-24">2023-06-24</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部\背部</p><p>核心项目：卧推 30~60kg * 5 杠铃划船、高位下拉 30~100kg * 5</p><hr><h4 id="2023-06-25">2023-06-25</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、面拉 20~40kg * 5</p><hr><h4 id="2023-06-26">2023-06-26</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 20~100kg * 5</p><hr><h4 id="2023-06-27">2023-06-27</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 胸部</p><p>核心项目：卧推 30~60kg * 5  哑铃上斜卧推突破30kg</p><hr><h4 id="2023-06-28">2023-06-28</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 背部</p><p>核心项目：杠铃划船、高位下拉 30~100kg * 5</p><hr><h4 id="2023-06-29">2023-06-29</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 肩部</p><p>核心项目：实力推、面拉 20~40kg * 5</p><hr><h4 id="2023-06-30">2023-06-30</h4><p>锻炼时间： 16:00 - 17:30</p><p>位置： 学校健身房</p><p>锻炼部位： 腿部</p><p>核心项目：深蹲 20~100kg * 5</p><hr>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
            <tag> Health </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-05锻炼日志</title>
      <link href="/posts/12336a0.html"/>
      <url>/posts/12336a0.html</url>
      
        <content type="html"><![CDATA[<hr><h4 id="2023-05-2">2023-05-2</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸部</p><p>练后体会： 无</p><hr><h4 id="2023-05-5">2023-05-5</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 腿部</p><p>练后体会： 无</p><hr><h4 id="2023-05-7">2023-05-7</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸部</p><p>练后体会： 无</p><hr><h4 id="2023-05-8">2023-05-8</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部</p><p>练后体会：</p><hr><h4 id="2023-05-9">2023-05-9</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 肩部</p><p>练后体会：</p><hr><h4 id="2023-05-10">2023-05-10</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 腿部</p><p>练后体会：</p><hr><h4 id="2023-05-13">2023-05-13</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸部</p><p>练后体会： 无</p><hr><h4 id="2023-05-14">2023-05-14</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部</p><p>练后体会：</p><hr><h4 id="2023-05-15">2023-05-15</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 肩部</p><p>练后体会：</p><hr><h4 id="2023-05-16">2023-05-16</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 腿部</p><p>练后体会：</p><hr><h4 id="2023-05-18">2023-05-18</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸部</p><p>练后体会： 无</p><hr><h4 id="2023-05-19">2023-05-19</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部</p><p>练后体会：</p><hr><h4 id="2023-05-21">2023-05-21</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 肩部</p><p>练后体会：</p><hr><h4 id="2023-05-22">2023-05-22</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 腿部</p><p>练后体会：</p><hr><h4 id="2023-05-24">2023-05-24</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸部</p><p>练后体会： 无</p><hr><h4 id="2023-05-25">2023-05-25</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部</p><p>练后体会：</p><hr><h4 id="2023-05-26">2023-05-26</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 肩部</p><p>练后体会：</p><hr><h4 id="2023-05-27">2023-05-27</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 腿部</p><p>练后体会：</p><hr><h4 id="2023-05-29">2023-05-29</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸部</p><p>练后体会： 无</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
            <tag> Health </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-04锻炼日志</title>
      <link href="/posts/dcb5ef25.html"/>
      <url>/posts/dcb5ef25.html</url>
      
        <content type="html"><![CDATA[<hr><h4 id="2023-04-1">2023-04-1</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 游泳馆</p><p>项目内容： 蛙泳练习</p><p>练后体会： 突破500m</p><hr><h4 id="2023-04-3">2023-04-3</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 无氧（胸部锻炼、肱三头肌锻炼）</p><p>练后体会： 纠正错误动作</p><hr><h4 id="2023-04-4">2023-04-4</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 游泳馆</p><p>项目内容： 200m</p><p>练后体会： 受伤…</p><hr><h4 id="2023-04-15">2023-04-15</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 复健</p><p>练后体会：</p><hr><h4 id="2023-04-18">2023-04-18</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 复健</p><p>练后体会：</p><hr><h4 id="2023-04-20">2023-04-20</h4><p>锻炼时间： 17:30 - 19:00</p><p>位置： 游泳馆</p><p>项目内容： 1km慢游</p><p>练后体会：</p><hr><h4 id="2023-04-24">2023-04-24</h4><p>锻炼时间： 16:00 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 综合复健</p><p>练后体会：</p><hr><h4 id="2023-04-25">2023-04-25</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 胸肌锻炼</p><p>练后体会：</p><hr><h4 id="2023-04-26">2023-04-26</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部锻炼</p><p>练后体会：</p><hr><h4 id="2023-04-27">2023-04-27</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部锻炼</p><p>练后体会：</p><hr><h4 id="2023-04-28">2023-04-28</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部锻炼</p><p>练后体会：</p><hr><h4 id="2023-04-29">2023-04-29</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 背部锻炼</p><p>练后体会：</p><hr>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
            <tag> Health </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2023-03锻炼日志</title>
      <link href="/posts/34701718.html"/>
      <url>/posts/34701718.html</url>
      
        <content type="html"><![CDATA[<hr><h4 id="2023-03-09">2023-03-09</h4><p>锻炼时间： 17:00 - 18:30</p><p>位置： 学校健身房</p><p>项目内容： 有氧热身（1km慢跑）、无氧（胸肌锻炼、臂肌锻炼）</p><p>练后体会： 手臂酸痛，抬不起来了QAQ</p><hr><h4 id="2023-03-10">2023-03-10</h4><p>锻炼时间： 17:00 - 18:30</p><p>位置： 学校健身房</p><p>项目内容： 有氧热身（1km慢跑）、无氧（肩部锻炼、背部锻炼）</p><p>练后体会： 整体感觉比昨天好，臂肌群还是有些酸痛</p><hr><h4 id="2023-03-14">2023-03-14</h4><p>锻炼时间： 17:00 - 18:30</p><p>位置： 学校健身房</p><p>项目内容： 有氧热身（1km慢跑）、无氧（肩部锻炼、腹部锻炼）</p><p>练后体会： 旧伤恢复得七七八八了，但是我是小趴菜~</p><hr><h4 id="2023-03-16">2023-03-16</h4><p>锻炼时间： 17:00 - 18:30</p><p>位置： 学校健身房</p><p>项目内容： 无氧（肩部锻炼、腹部锻炼 、背部锻炼）</p><p>练后体会： 轻松~</p><hr><h4 id="2023-03-18">2023-03-18</h4><p>锻炼时间： 16:30 - 18:30</p><p>位置： 学校健身房</p><p>项目内容： 有氧热身（1km慢跑）、无氧（肩部锻炼、胸部锻炼）</p><p>练后体会： 适应且习惯了，很舒服</p><hr><h4 id="2023-03-19">2023-03-19</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 有氧热身（1.5km慢跑）、无氧（肩部锻炼、肱二锻炼）</p><p>练后体会： 到位</p><hr><h4 id="2023-03-21">2023-03-21</h4><p>锻炼时间： 16:30 - 18:00</p><p>位置： 学校健身房</p><p>项目内容： 无氧（胸部锻炼、肱三锻炼）</p><p>练后体会： 肌无力？？？</p><hr><h4 id="2023-03-22">2023-03-22</h4><p>锻炼时间： 17:30 - 19:30</p><p>位置： 游泳馆</p><p>项目内容： 游泳</p><p>练后体会： 有进步，且感觉很舒服</p><hr><h4 id="2023-03-23">2023-03-23</h4><p>锻炼时间： 17:30 - 19:30</p><p>位置： 学校健身房</p><p>项目内容： 无氧（胸部锻炼、肩部锻炼、肱二头肌锻炼）</p><p>练后体会： 划船貌似有点行，但是卧推是真不行</p><hr><h4 id="2023-03-25">2023-03-25</h4><p>锻炼时间： 14:30 - 16:30</p><p>位置： 游泳馆</p><p>项目内容： 蛙泳练习</p><p>练后体会： 学习中</p><hr><h4 id="2023-03-26">2023-03-26</h4><p>锻炼时间： 14:30 - 16:30</p><p>位置： 游泳馆</p><p>项目内容： 蛙泳练习</p><p>练后体会： 学习中</p><hr><h4 id="2023-03-28">2023-03-28</h4><p>锻炼时间： 17:30 - 19:00</p><p>位置： 游泳馆</p><p>项目内容： 蛙泳练习</p><p>练后体会： 高速泳道浮力感觉会更大一些</p><hr><h4 id="2023-03-29">2023-03-29</h4><p>锻炼时间： 17:30 - 19:00</p><p>位置： 学校健身房</p><p>项目内容： 无氧（胸部锻炼、肩部锻炼、肱二头肌锻炼）</p><p>练后体会： 有些许吃力 很久没练</p><hr><h4 id="2023-03-30">2023-03-30</h4><p>锻炼时间： 17:30 - 19:30</p><p>位置： 游泳馆</p><p>项目内容： 蛙泳练习</p><p>练后体会： 200m比较轻松</p><hr><h4 id="2023-03-31">2023-03-31</h4><p>锻炼时间： 16:30 - 19:00</p><p>位置： 学校健身房</p><p>项目内容： 无氧（胸部锻炼、肩部锻炼）</p><p>练后体会： 卧推…</p><hr>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
            <tag> Health </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>新冠感染及痊愈日记</title>
      <link href="/posts/e28b8d84.html"/>
      <url>/posts/e28b8d84.html</url>
      
        <content type="html"><![CDATA[<h3 id="第一天-2022-12-12"><strong>第一天 2022.12.12</strong></h3><p>其实第一天并没有什么特别的感觉，就只是有一点嗓子痛，疼痛程度甚至不及一般的扁桃体发炎带来的痛感，又加之当晚去全家买了“海氏海诺”牌的电子体温计，自测的温度从来都没上过37度。所以我就以为就是普通的扁桃体发炎，更甚着大不了就是得了流行性感冒。</p><h3 id="第二天-2022-12-13"><strong>第二天 2022.12.13</strong></h3><p>这天上午不知道为什么变得特别困，根本睡不醒，早上九点过就起来了，然后因为一系列原因，继续睡到了12点，这时间起来的时候我又测了一次体温，电子体温计显示的是35.8，我就放心的出门了，其实在出门的时候，我女朋友就已经感觉我脸有点轻微发烫了，但是我们还是选择了相信体温计。</p><p>中午在中快餐厅打完饭之后，我自己带到了实验室吃，我女朋友拿回家吃的，我吃完之后大概一点，然后就想着休息会就开始工作，大概过了半个小时，我就觉得脸部特别烫，一摸额头也发烧了，我就知道我大概率中招了，我就立刻赶到校医院发热门诊进行就诊。在经过两个小时的排队等候之后，我就得到了一盒“氨麻美敏片”，医生就叫我回家去休息。其实我这个时候的发烧已经非常严重了，我在等候的时候，自己测了好几次体温（这时候是从实验室拿的水银温度计了），一次比一次高，最高的甚至到了38.9。</p><p>晚上八点我吃了一粒布洛芬，然后在实验室开始长达10个小时的痛苦折磨。吃完药之后，我的体温并没有立刻得到控制，温度还是在慢慢上升，八点半的时候我女朋友买的自测试剂盒到了，我阳性，她阴性。我让她自己回家，我选择在实验室进行隔离，因为她还有11天就要考研了，我不想因为我传染给她新冠导致影响考研的发挥。这时候我就一个人在实验室呆着了，女朋友给我拿来了生活用品，但基本我没怎么用，因为我轻微一动，整个脑子就像要裂开了一样的疼痛，我只敢躺着一动不动。</p><p>在晚上22点20分之后，我开始试图睡觉，很快就睡着了，但是由于在躺椅上睡的，本身并不是很舒服，又加之一直在发烧，我凌晨1点半的时候就醒来了，我又测了一次温度，36.9。我以为我已经退烧了，症状的到了好转，就起来接了杯水喝下了，可能是因为动了，脑子又开始剧痛难忍，再一次睡觉了之后，六点钟就醒了。我测了一次体温，还是38.3。还是处于发烧期，我准备七点的时候吃一粒布洛芬，然后看看发烧能否缓解。</p><hr><h4 id="2023年三月五日更新：">2023年三月五日更新：</h4><p>突然想起来这个还没写完，大概后面就是几天就转阴了，但是断断续续肺炎了一个多月，又是输液又是吃药，还蛮难受的。</p><p>最近我们这里又在传说什么甲流，哈人！！！！带好口罩吧~</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
            <tag> Health </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入理解Faster RCNN</title>
      <link href="/posts/451f615a.html"/>
      <url>/posts/451f615a.html</url>
      
        <content type="html"><![CDATA[<h3 id="一、简介">一、简介</h3><p>该论文作为目标检测中二阶段算法极具代表性的文章，提出了基于anchor的目标检测器，抛弃了传统的通过滑动窗口、SS（Selective Search）等方法生成目标候选框的方法，使用RPN直接生成目标候选框，极大地提升了目标候选框的生成速度。放到如今的2023年，虽然有视觉Transformer的强势冲击，RPN仍然是目标检测领域一个主力军。</p><p><a href="https://arxiv.org/abs/1506.01497">论文传送门</a></p><h3 id="二、网络结构">二、网络结构</h3><h4 id="1-结构总览">1. 结构总览</h4><p>整个Faster RCNN网络的结构可以分为<strong>四个大块</strong>：<strong>backbone、RPN、RoI、Head</strong>。</p><ol><li>backbone可以任意根据新的好的主干网络进行更换，主要用来进行输入图像的特征提取，现在甚至可以将其替换成基于transformer的ViT、Swim transformer之类的，这里不做过多赘述。</li><li>RPN（Region proposal network）作用是进行建议区域的提取网络，在特征图中先生成一系列的anchors，然后通过边界框回归等一系列操作，生成最终修正过的anchors。</li><li>RoI（Region of Interest）作用是将anchors映射回原特征图，并进行最大池化，得到固定长度的特征图。</li><li>Head，通过将固定长度的特征图进行FC之后进行类别softmax和BBox 回归。</li></ol><p>整个网络的前向传播过程也是按照这四个模块的顺序进行传播的，粗略来说就是首先输入一张（一批）图片，经过backbone得到输出为输入图像的特征图，然后将特征图输入到RPN中，得到的输出为特征图的区域建议框（anchor），并且会在这一步将从特征图上选取的一系列区域建议框映射到原特征图的位置中，并在这一步将边缘的anchor的边框与特征图的边框进行对齐（即在边缘的anchor可能出现超出图片边缘的边框，则将图片边缘当做该anchor的新边界）。然后将对应好的anchors作为输入进入RoI层， 得到的输出为固定长度的proposal feature maps。最后就是Head，将proposal feature maps作为输入，经过全连接层，进行分类和边界框回归，得到最终结果，其网络结构如图2.1所示。</p><center><img src= 'https://imagebed-2jk.pages.dev/img/fasterrcnn.png' width ='85%'></center><center>图2.1 Faster RCNN结构图</center><h4 id="2-Backbone">2. Backbone</h4><p>原文使用到的backbone为VGG16，这里对VGG16不做详细介绍，因为backbone在后续的研究可以被随意更换，Pytorch官方更是实现了Faster RCNN的多个backbone版本，可以在<a href="https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py">官方代码</a>中查看。</p><h4 id="3-RPN">3. RPN</h4><p>RPN是Faster RCNN系列中最重要的一个模块，作者后续的Mask RCNN版本中也仍然沿用了这一设计，RPN的结构如图2.2所示。</p><center><img src='https://imagebed-2jk.pages.dev/img/RPN.png' width='70%'></center><center>图2.2 RPN结构图</center><p>图2.2中红框箭头的输入则是通过backbone提取到的特征图，首先会通过红框中所示的Relu(Conv2d())，产生两个分支，上面的分支用于anchor的产生，判断哪些anchor是positive哪些是negative的，下面的分支用于计算anchor的BBox regression的偏移量，用来获取精确的proposal。</p><h4 id="4-RoI">4. RoI</h4><h4 id="5-Head">5. Head</h4>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>einsum用法与理解</title>
      <link href="/posts/c1c48943.html"/>
      <url>/posts/c1c48943.html</url>
      
        <content type="html"><![CDATA[<h3 id="1-简介">1. 简介</h3><p>爱因斯坦求和约定（einsum）提供了一套既简洁又优雅的规则，可实现包括但不限于：向量内积，向量外积，矩阵乘法，转置和张量收缩等张量操作，熟练运用 einsum 可以很方便的实现复杂的张量操作，而且不容易出错。通常情况下，可以在计算效果上等同于pytorch中的矩阵乘法运算等，且书写方式较为随意，主要是从理解层面进行操作。在Python中的引入如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="comment">#只用下面一行即可引入einsum包，这里是为了下面的矩阵定义做准备</span></span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> einsum</span><br></pre></td></tr></table></figure><h3 id="2-矩阵乘法、转置用法及计算原理">2. 矩阵乘法、转置用法及计算原理</h3><p>如果你已经明白einsum中的维度对应关系，可以直接跳过2.1和2.2部分，直接开始看2.3部分。</p><h4 id="2-1-入门理解">2.1 入门理解</h4><p>为了讲明白einsum的用法及计算原理，我们首先定义几个矩阵，从几个例子入手：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn([<span class="number">3</span>，<span class="number">5</span>])</span><br><span class="line">b = torch.randn([<span class="number">5</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure><p>用einsum实现a和b之间的矩阵乘法操作，如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mutiple = einsum(<span class="string">&#x27;ik,kj -&gt; ij&#x27;</span>, a, b)</span><br><span class="line"><span class="built_in">print</span>(mutiple.shape) <span class="comment"># result:[3,6]</span></span><br></pre></td></tr></table></figure><p>用einsum实现a矩阵转置操作，如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">trans = einsum(<span class="string">&#x27;ij -&gt; ji&#x27;</span>, a)</span><br><span class="line"><span class="built_in">print</span>(trans.shape) <span class="comment"># result:[5,3]</span></span><br></pre></td></tr></table></figure><p>没错，你是否已经观察出了一些规律呢？那我们来验证一下你的观察结果是否正确，看下面的代码，猜猜看输出会是什么结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c = torch.randn([<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">d = torch.randn([<span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">guess_1 = einsum(<span class="string">&#x27;bhqd,bhdk -&gt; bhqk&#x27;</span>, c, d)</span><br><span class="line">guess_2 = einsum(<span class="string">&#x27;bhqd -&gt; bdhq&#x27;</span>, c)</span><br><span class="line">guess_3 = einsum(<span class="string">&#x27;bhqd,bhkd -&gt; bhkq&#x27;</span>, c, d)</span><br><span class="line"><span class="built_in">print</span>(guess_1.shape)</span><br><span class="line"><span class="built_in">print</span>(guess_2.shape)</span><br><span class="line"><span class="built_in">print</span>(guess_3.shape)</span><br></pre></td></tr></table></figure><p>现在验证一下你的猜测是否正确：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">guess_1.shape = torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">guess_2.shape = torch.Size([<span class="number">2</span>, <span class="number">5</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">guess_3.shape = torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure><h4 id="2-2-维度理解">2.2 维度理解</h4><p>我们来理解一下上述的三个guess，它的结果到底是怎么计算出来的。</p><ol><li>guess_1: 很简单，按照之前的规律，bhqd、bhdk 指代 c、d 两个输入矩阵的原始维度。也就是说，在 -&gt; 的左边部分，bhqd 四个字母分别表示维度2 2 4 5，bhdk 四个字母分别表示维度2 2 5 5，那么 -&gt; 右边的 bhqk 应该对应着2 2 4 5。</li><li>guess_2: 同上，-&gt; 左边的 bhqd 表示维度 2 2 4 5，-&gt; 右边的 bdhq 则是将几个维度调换了位置，结果维度通过字母指代的值对应起来，正确答案自然而然是：2 5 2 4。</li><li>guess_3：我们仍然将 -&gt; 左边和右边的字母指代的维度作比较，很eazy就能得出结论：2 2 5 4。</li></ol><hr><p>看到这里我们发现的规律应该是这样：<br>在einsum函数里，我们需要关注的分为两部分，第一部分为字符串，如<code>'bhqd,bhdk -&gt; bhqk'</code>，第二部分为后面跟着的参数，如 c 和 d 。并且我们对字符串中的理解仍然分为两部分，这两部分由 -&gt; 划分开。且 -&gt; 左边的部分由逗号隔开，分别与参数一一对应。进一步理解，字符串中的每一个字符都指代其对应参数的某一个维度值。例如：<code>einsum('bhqd,bhdk -&gt; bhqk', c, d)</code>中的 bhqd 分别对应着 c 矩阵的四个维度2 2 4 5，b 矩阵的对应替代关系同理。然后根据这种对应关系来算出 -&gt; 右边的字母对应的维度，即为该函数返回的变量维度。<br>看到这里，你已经能准确判断出einsum函数的维度变换关系了，接下来该看看它计算的结果了。</p><h4 id="2-3-内部计算">2.3 内部计算</h4><p>要探究它内部的计算关系，我们还是回到第一个例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn([<span class="number">3</span>，<span class="number">5</span>])</span><br><span class="line">b = torch.randn([<span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">mutiple = einsum(<span class="string">&#x27;ik,kj -&gt; ij&#x27;</span>, a, b)</span><br></pre></td></tr></table></figure><p>为什么说上述式子就实现了矩阵的乘法呢，那是因为该函数的三条基本准则：</p><ol><li>在 -&gt; 左边不同输入之间重复出现的索引（索引即是英文字母）表示把输入张量沿着该维度做乘法操作，比如<code>einsum('ik,kj -&gt; ij', a, b)</code>，k 在输入中重复出现，所以就是把 a 和 b 沿着 k 这个维度作相乘操作。</li><li>只出现在 -&gt; 左边的索引，表示中间计算结果需要在这个维度上求和，也就是上面例子中的 k。</li><li>在 -&gt; 右边的索引顺序可以是任意的，比如上面的<code>'ik,kj-&gt;ij'</code>如果写成<code>'ik,kj-&gt;ji'</code>，那么就是返回输出结果的转置，用户只需要定义好索引的顺序，转置操作会在 einsum 内部完成。</li></ol><p>是不是觉得云里雾里，没关系，我们先不管这个准则，回到我们的三个guess里。为了方便观看，我将其贴到这里来：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">c = torch.randn([<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">d = torch.randn([<span class="number">2</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">5</span>])</span><br><span class="line">guess_1 = einsum(<span class="string">&#x27;bhqd,bhdk -&gt; bhqk&#x27;</span>, c, d)</span><br><span class="line">guess_2 = einsum(<span class="string">&#x27;bhqd -&gt; bdhq&#x27;</span>, c)</span><br><span class="line">guess_3 = einsum(<span class="string">&#x27;bhqd,bhkd -&gt; bhkq&#x27;</span>, c, d)</span><br><span class="line"><span class="built_in">print</span>(guess_1.shape) <span class="comment"># [2, 2, 4, 5]</span></span><br><span class="line"><span class="built_in">print</span>(guess_2.shape) <span class="comment"># [2, 5, 2, 4]</span></span><br><span class="line"><span class="built_in">print</span>(guess_3.shape) <span class="comment"># [2, 2, 5, 4]</span></span><br></pre></td></tr></table></figure><p>我们将上述 bhqd 理解为四个索引，即通过这四个索引值便可以定位 c 矩阵的任意位置。（举个例子，我们可以理解为<code>c = torch.randn([2, 2, 4, 5])</code>是一层书架，它的四个维度分别为 bhqd，b 表示这层书架上的书本数量为2，h 理解为每本书一共有2页，q 表示每一页书一共有4行字，d 表示每一行字一共有5个字。那么我们就可以通过一个 bhqd 索引来确定这一层书架上的每一个字。读者以后也可以通过这种方式理解高维矩阵，如果是五维，可以再加一个维度是书架层数等）<br>那么guess_1矩阵的 bhqk 也是它的索引，而且这个索引是通过 c 矩阵的 bhqd 与 d 矩阵的 bhdk 沿着 d 这个维度（这里为什么是沿着 d 维度，参照上面三条基本准则的第一条）做内积得到的。用数学公式表达则是：</p><p>$$guess1_{[b,h,q,k]}= \sum\limits_{i=0}^{d}c_{[b,h,q,i]}*d_{[b,h,i,k]}$$</p><p>guess_1中提到的<code>'bhqd,bhdk -&gt; bhqk'</code>通过直观的理解，就是在 qd 维度上与 dk 维度相乘，自然而然得到了 qk。也符合矩阵乘法的运算规律。<br>guess_2中不涉及到运算，只是针对维度的变换，可以参照上面关于书架的假设来理解。<br>guess_3中提到的<code>'bhqd,bhkd -&gt; bhqk'</code>虽然也是对后两个维度进行乘法操作，但是书写方式与我们常规理解的矩阵乘法有出入（通常的书写方式为$ [i,k]*[k,j] = [i,j] $）,我们可以用数学公式帮助理解：</p><p>$$guess3_{[b,h,k,q]}= \sum\limits_{i=0}^{d}c_{[b,h,q,i]}*d_{[b,h,k,i]}$$</p><p>下图是对guess_3公式中索引含义的进一步解释，因为 b 和 h 可以理解为多几个维度的切片参与计算，所以可以先理解成二维矩阵之间相乘，再拓展到高维的多层矩阵乘法即可。假设guess_3中的索引值为[b h 3 2]，从guess_3的求和公式可以得出，是 c 矩阵的 q 维度为2，d 矩阵的 k 维度为3，然后再从 i=0 开始对 d 维度做内积并求和。对应到图中则是 c 矩阵中橙色的一整行与 d 矩阵的粉色一整行对应位置相乘并相加。这是二维之间相乘，对应到高维，则是 c 矩阵中后续通道中对应橙色位置的一整行与 d 矩阵中后续通道中对应粉色位置的一整行相乘再全部相加，得到最终的guess_3矩阵中后续通道的紫色位置的值。同理，通过这样的乘法方式计算出guess_3矩阵每个位置的结果即可。</p><img src='https://imagebed-2jk.pages.dev/img/2023-03-03-einsum用法与理解_1.jpg' width='70%'><center>图1. guess_3中矩阵乘法可视化</center>]]></content>
      
      
      <categories>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>A survey of modern Deep learning based Object Detection Models</title>
      <link href="/posts/324ed694.html"/>
      <url>/posts/324ed694.html</url>
      
        <content type="html"><![CDATA[<h3 id="1-简介">1. 简介</h3><p>​目标检测通常是指在图片或视频中对目标物体的边界框以及类别做出精准判断的任务。目标检测包含了两方面的内容：对物体边界框的回归和对已有边界框的物体进行分类。目前目标检测领域在应用到实际生活的过程中，仍然存在以下三个关键问题：</p><ul><li>类内差异（Intra class variation）：在普通现实场景中，出现类内差异的情况非常常见。例如同一种类别的物体可能会出现遮挡、重叠、光照、姿势、视点等变化，而这种外界因素的干扰或者约束，可能会给同一种物体的外观带来巨大的差异，甚至有些物体可能出现非刚性的变形或者旋转、缩放或者模糊等变化。这些上述的原因都将给物体的提取识别带来困难。</li><li>类别的数量（Number of categories）：在一张图片中往往会出现不止一个同一类别的物体，这不光对于检测器的精度要求非常高，并且对数据的标注要求也非常高。</li><li>效率（Efficiency）：目前已有目标检测模型成功应用到实时检测系统里，因此检测的速度也成为了一项重要的评价指标。</li></ul><p>​目标检测领域正是以解决上述三个难点逐渐发展起来的，现有的优秀目标检测器有非常之多，以经典的一阶段、二阶段检测算法为代表，近几年又兴起了以Transform为基础的无卷积网络。本文将针对目标检测器的显著分类进行介绍。与此同时，本文也将介绍目标检测领域使用广泛的一些评价指标和数据集。</p><h3 id="2-数据集">2. 数据集</h3><p>​现有的目标检测数据集主要有以下几类：</p><h4 id="1）PASCAL-VOC-07-12">1）PASCAL VOC 07/12</h4><p>​The Pascal Visual Object Classes (VOC) 挑战是一项多年的努力，旨在加速视觉感知领域的发展。它开始于2005年，起初只有四个物体类别的分类和检测任务，但这个挑战的两个版本（07和12版本）大多被用作标准基准，其简单信息如下：</p><p>​07：训练集（5011幅），测试集（4952幅），共计9963幅图，共包含20个种类。<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar">下载链接</a></p><p>​12：训练集（5717幅），验证集（5823幅），训练集+验证集共计11540幅图，共包含20个种类（main文件夹下）。<a href="http://host.robots.ox.ac.uk/pascal/VOC/voc2012/index.html#devkit">下载链接</a></p><p>​Pascal VOC引入了在0.5 IoU（Intersection over Union）的平均平均精度（mAP）来评估模型的性能。</p><h4 id="2）ILSVRC">2）ILSVRC</h4><p>​The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) 是一项从2010年到2017年年度举办的挑战赛。ImageNet作为该挑战赛的官方数据集，是由斯坦福著名教授李飞飞等人创建的，该数据集包含了超过50万张共计200类的目标检测任务图片。且自2010年起，随着比赛的进行，该数据集每年都会进行完善更新，所以下载的时候请注意下载对应年份版本的数据集。</p><p>​由于该数据集不允许作为商用，完整下载该数据集需要edu邮箱验证，因此只在此放上官网链接，进行注册验证使用。<a href="https://image-net.org/challenges/LSVRC/index.php">ImageNet官网</a></p><p>​如果因为网络原因下载很慢的话，不妨试试<a href="https://academictorrents.com/details/fbc7a9f9a10be134a1738ba947efa1814ed3ce9b">这个链接</a>。</p><h4 id="3）MS-COCO">3）MS-COCO</h4><p>​The Microsoft Common Objects in Context (MS-COCO) 是著名的挑战赛数据集之一，由微软公司创建提供。该数据集包含超过30万张共计80类的目标检测任务图片，且平均每张图片中包含了3.5个类别，7.7个目标实例，目标的密集度大于其他著名数据集。并且，MS COCO数据集中的图片来自不同的视角。<a href="https://cocodataset.org/#download">下载链接</a></p><p>​与Pascal VOC和ILSVCR不同，它从0.5到0.95分步计算IoU，然后用这10个值的组合作为最终指标，称为Average Precision (AP)。</p><p>​MS-COCO官网目前无法下载数据集，可以尝试用下面的命令行进行下载：</p><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mkdir coco</span><br><span class="line">cd coco</span><br><span class="line">mkdir images</span><br><span class="line">cd images</span><br><span class="line"></span><br><span class="line">wget -c http:<span class="regexp">//im</span>ages.cocodataset.org<span class="regexp">/zips/</span>train2017.zip</span><br><span class="line">wget -c http:<span class="regexp">//im</span>ages.cocodataset.org<span class="regexp">/zips/</span>val2017.zip</span><br><span class="line">wget -c http:<span class="regexp">//im</span>ages.cocodataset.org<span class="regexp">/zips/</span>test2017.zip</span><br><span class="line"></span><br><span class="line">wget -c http:<span class="regexp">//im</span>ages.cocodataset.org<span class="regexp">/annotations/</span>annotations_trainval2017.zip</span><br><span class="line">wget -c http:<span class="regexp">//im</span>ages.cocodataset.org<span class="regexp">/annotations/</span>stuff_annotations_trainval2017.zip</span><br><span class="line">wget -c http:<span class="regexp">//im</span>ages.cocodataset.org<span class="regexp">/annotations/im</span>age_info_test2017.zip</span><br></pre></td></tr></table></figure><h4 id="4）Open-Image">4）Open Image</h4><p>​谷歌的Open Image数据集由920万张图像组成，其中包含190万张图片共计600类的目标检测任务图片。该数据集对Pascal VOC中引入的AP做了一些改变，如忽略未注释的类，对类和其子类的检测要求等。<a href="https://storage.googleapis.com/openimages/web/download_v7.html">下载链接</a></p><p>​</p><p>​根据上述四个数据集的类别分布分析得出：<strong>这些数据集大部分都存在类别不平衡的问题</strong>，即不同类别的图片数量差异较大。这可能会导致训练出的模型更倾向于识别图片数量大的类别，而对于图片数量较少的类别，识别的可能性较小或者准确率较低。但是上述问题不太会出现在ImageNet数据集中，ImageNet数据集中的类别数量比较平均，数量最大的类为“考拉”，一共有2469张，数量最小的类为“手推车”，也有624张。但是ImageNet中数量第二的类别为“手推车”，这可能会导致另外一个问题：现实世界的物体检测场景中，“考拉”“手推车”等物体并不是最受欢迎的，相较于此，“人”“树木”“车”等物体出现的频率可能更高。这中数据集分布于现实场景仍存在差异，这也是目前无法解决的问题。</p><h3 id="3-Backbone">3. Backbone</h3><p><img src="https://imagebed-2jk.pages.dev/img/A-survey-of-modern-Deep-learning-based-Object-Detection-Models_Backbone.png" alt="图3.1 Backbones"></p><div align = "center">图3.1 Backbones</div><h3 id="4-Object-detectors">4. Object detectors</h3><p><img src="https://imagebed-2jk.pages.dev/img/A-survey-of-modern-Deep-learning-based-Object-Detection-Models_Detector.png" alt="图4.1 部分经典目标检测器"></p><div align = "center">图4.1 部分经典目标检测器</div><p>​</p><h3 id="5-Breakpoint">5. Breakpoint</h3><p>​本文后续还有关于轻量目标检测模型的介绍、各个检测器对比的结果、未来趋势的展望。</p><p>​但是，笔者意图做目标检测的简单介绍。感兴趣的读者可以<a href="https://arxiv.org/abs/2104.11892">点击此处</a>查看全文。遂作此断点，未来再读。</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git入门教程</title>
      <link href="/posts/29207ea7.html"/>
      <url>/posts/29207ea7.html</url>
      
        <content type="html"><![CDATA[<h1>Git</h1><h2 id="一、基本原理">一、基本原理</h2><p>教程来源：</p><p><a href="https://www.bilibili.com/video/BV1r3411F7kn?spm_id_from=333.337.search-card.all.click&amp;vd_source=3aa92f173cf789aed74021ca69519fcd">Git工作流和核心原理 | GitHub基本操作 | VS Code里使用Git和关联GitHub_哔哩哔哩_bilibili</a></p><center><img src='https://imagebed-2jk.pages.dev/img/Git_Untitled.png' width='80%'></center><h2 id="二、使用教程">二、使用教程</h2><h3 id="本地Git使用命令"><strong>本地Git使用命令</strong></h3><table><thead><tr><th>git config —global <a href="http://user.name">user.name</a> “用户名”</th><th>设置用户名</th></tr></thead><tbody><tr><td>git config —global user.email “邮箱”</td><td>设置邮箱</td></tr><tr><td>git init</td><td>初始化文件夹为git文件工作区</td></tr><tr><td>git status</td><td>查看当前工作区的状态</td></tr></tbody></table><table><thead><tr><th>git add 文件名</th><th>添加文件</th></tr></thead><tbody><tr><td>git commit -m “版本信息”</td><td>提交文件</td></tr><tr><td>git log</td><td>查看往期版本</td></tr><tr><td>git commit -a -m “版本信息“</td><td>光速提交</td></tr><tr><td>touch .gitignore</td><td>创建要忽略的文件夹，即不会被添加到工作区的文件。需要把不提交的文件名写到.gitignore文件里</td></tr></tbody></table><table><thead><tr><th>git branch 分支名</th><th>创建新的分支</th></tr></thead><tbody><tr><td>git checkout 分支名</td><td>切换到指定分支</td></tr><tr><td>git branch -d 分支名</td><td>删除分支</td></tr><tr><td>git checkout -b 分支名</td><td>创建新的分支并切换到新分支</td></tr><tr><td>git merge 分支名</td><td>把分支名的分支合并到当前所处的分支</td></tr></tbody></table><h3 id="Github结合git使用">Github结合git使用</h3><table><thead><tr><th>git clone HTTPS</th><th>从github下载到本地</th></tr></thead><tbody><tr><td>git remote -v</td><td>查看当前本地仓库和哪些远程仓库有联系</td></tr><tr><td>git remote add origin <a href="mailto:git@github.com">git@github.com</a>….</td><td>把本地仓库与远程仓库关联</td></tr><tr><td>git push -u origin master</td><td></td></tr><tr><td>git fetch</td><td>拉远程仓库到暂存</td></tr><tr><td>git pull</td><td></td></tr><tr><td>git rm -r -n 文件夹 —cached</td><td>删除远程仓库里的文件夹但不删除本地的，如果不加—cached，本地里的也会删掉。如果不加-n会直接删掉，加了的话会先预览删除的文件。</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> Environments </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 实用工具 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch入门级深度学习框架搭建</title>
      <link href="/posts/f700e2b8.html"/>
      <url>/posts/f700e2b8.html</url>
      
        <content type="html"><![CDATA[<h1>Torch</h1><h1>一、教程地址</h1><p><a href="https://www.bilibili.com/video/BV1hE411t7RN?p=8&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】_哔哩哔哩_bilibili</a></p><hr><h1>二、深度学习实战</h1><h2 id="（一）Dataset">（一）Dataset</h2><h3 id="1-引用">1.引用</h3><p>from <a href="http://torch.utils.data">torch.utils.data</a> import Dataset</p><h3 id="2-必须实现的方法：">2.必须实现的方法：</h3><table><thead><tr><th><strong>init</strong>(self):</th><th>初始化方法，通常用来初始化一些路径参数</th></tr></thead><tbody><tr><td><strong>getitem</strong>(self, idx):</td><td>获取一个数据的方法。如果为图像数据，通常返回一个img和一个label对象</td></tr><tr><td><strong>len</strong>(self):</td><td>返回数据集的大小</td></tr></tbody></table><h3 id="3-代码">3.代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, label_dir</span>):</span><br><span class="line">        self.root_dir = root_dir</span><br><span class="line">        self.label_dir = label_dir</span><br><span class="line">        self.path = os.path.join(self.root_dir, self.label_dir)</span><br><span class="line">        self.img_path = os.listdir(self.path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        label = self.label_dir</span><br><span class="line">        img = Image.<span class="built_in">open</span>(os.path.join(self.path, self.img_path[idx]))</span><br><span class="line">        <span class="keyword">return</span> img, label</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.img_path)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    root_dir = <span class="string">&#x27;hymenoptera_data/train&#x27;</span></span><br><span class="line">    ants_label_dir = <span class="string">&#x27;ants&#x27;</span></span><br><span class="line">    bees_label_dir = <span class="string">&#x27;bees&#x27;</span></span><br><span class="line">    ants_dataset = MyDataset(root_dir, ants_label_dir)</span><br><span class="line">    bees_dataset = MyDataset(root_dir, bees_label_dir)</span><br></pre></td></tr></table></figure><h2 id="（二）Tensorboard">（二）Tensorboard</h2><h3 id="1-引用-2">1.引用</h3><p>from torch.utils.tensorboard import SummaryWritter</p><h3 id="2-常用的方法">2.常用的方法</h3><table><thead><tr><th>方法名</th><th>备注</th></tr></thead><tbody><tr><td>add_scalar(self,tag,scalar_value,global_step)</td><td>这是常用的用来写数据的方法。ag是图名，scalar_value是纵坐标，global_step是横坐标</td></tr><tr><td>add_image(self,tag,img_tensor,global_step,dataformats)</td><td>这是用来添加图片的方法。img_tensor需要是numpy或者tensor类型,dataformats用来指定numpy类型图片的格式是‘HWC’还是’CHW’</td></tr><tr><td>close()</td><td>关闭，通常是必须的代码</td></tr></tbody></table><h3 id="3-使用">3.使用</h3><p>控制台使用代码：tensorboard —logdir=logs —port=6006</p><p>如果是服务器中有很多训练的话，可以使用—port指定不同的端口来显示。</p><h3 id="4-代码">4.代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">tb = SummaryWriter(<span class="string">&#x27;log&#x27;</span>)</span><br><span class="line">path = <span class="string">&#x27;hymenoptera_data/train/bees/21399619_3e61e5bb6f.jpg&#x27;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(path)</span><br><span class="line">img = np.array(img)</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line">tb.add_image(<span class="string">&#x27;test2&#x27;</span>, img, <span class="number">1</span>, dataformats=<span class="string">&#x27;HWC&#x27;</span>)</span><br><span class="line"><span class="comment"># for i in range(10000):</span></span><br><span class="line"><span class="comment">#     tb.add_scalar(&#x27;y=x+x^2&#x27;, i + i * i, i)</span></span><br><span class="line">tb.close()</span><br></pre></td></tr></table></figure><h2 id="（三）Transform">（三）Transform</h2><h3 id="1-引用-3">1.引用</h3><p>from torchvision import transforms</p><h3 id="2-含义及使用">2.含义及使用</h3><p>tensor：是一个包装了训练所需参数的一种数据类型</p><p>transforms类似于一个工具箱，里面有很多类，作为工具使用。所以在需要使用一个工具时，需要先实例化一个工具的对象然后再使用。比如ToTensor的使用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()</span><br><span class="line">img_tensor = tensor_trans(Image.<span class="built_in">open</span>(<span class="string">&#x27;&#x27;</span>))</span><br></pre></td></tr></table></figure><img src='https://imagebed-2jk.pages.dev/img/Pytorch_Untitled.png' width='70%'><p>​transfroms工具的使用流程</p><h3 id="3-常见的transforms">3.常见的transforms</h3><p>（1）图片打开方式</p><table><thead><tr><th>Python包</th><th>打开方法</th><th>数据类型</th></tr></thead><tbody><tr><td>from PIL import Image</td><td>Image.open()</td><td>JPEGImgFile</td></tr><tr><td>from torchvision import transforms</td><td>ToTensor()</td><td>tensor</td></tr><tr><td>import cv2</td><td>cv2.imread()</td><td>numpy(ndarray)</td></tr></tbody></table><p>（2）transforms中的类</p><table><thead><tr><th>类名</th><th>参数</th><th>作用</th></tr></thead><tbody><tr><td>Compose</td><td>一个列表[transforms1,transforms2,…]</td><td>组合transforms，可看成串联操作</td></tr><tr><td>ToTensor</td><td>pic: PIL or numpy.ndarray</td><td>把PIL格式或者numpy格式的图像转换成一个tensor类型的数据</td></tr><tr><td>Normalize</td><td>mean: [float,float,float]  std: [float,float,float]</td><td></td></tr><tr><td>ToPILImage</td><td></td><td></td></tr><tr><td>Resize</td><td>object: 可以为(512,512)</td><td>把PIL图像resize到给定的尺寸</td></tr><tr><td>RandomCrop</td><td>object: 可以为(512,512)</td><td>在PIL中随机裁剪size尺寸的图片并返回</td></tr></tbody></table><h2 id="（四）公共数据集">（四）公共数据集</h2><h3 id="1-引用-4">1.引用</h3><p>import torchvision</p><h3 id="2-使用">2.使用</h3><p>torchvision.dataset.数据集名称</p><p>包含参数：root:str    train=True|False  download=True|False  transform=(这里是需要自己预先设定好transforms操作的)</p><h2 id="（五）Dataloader">（五）Dataloader</h2><h3 id="1-实例代码">1.实例代码</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(root=<span class="string">&#x27;./dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>,</span><br><span class="line">                                         transform=torchvision.transforms.ToTensor(), download=<span class="literal">True</span>)</span><br><span class="line">test_loader = DataLoader(dataset=test_data, batch_size=<span class="number">4</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>, drop_last=<span class="literal">False</span>)</span><br><span class="line">img, target = test_data[<span class="number">0</span>]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br></pre></td></tr></table></figure><h3 id="2-理解">2.理解</h3><p>dataloader可以理解为一个取数据的容器，按照batch_size的大小从数据集里每次打包一个batch的数据，并且是分别将img和target分开打包的。</p><h2 id="（六）nn-Module">（六）nn.Module</h2><h3 id="1-理解">1.理解</h3><p>可以理解为一个网络模板，只需要进行一定的修改，便可以形成自己的网络模型</p><h3 id="2-用法">2.用法</h3><p>通常是定义自己的modul类并继承nn.Module类，然后实现其中的方法</p><p>具体的Module类可以参看下面的官方文档：</p><p><a href="https://pytorch.org/docs/stable/generated/torch.nn.Module.html#module">Module - PyTorch 1.11.0 documentation</a></p><h3 id="3-代码-2">3.代码</h3><p><strong>二维卷积</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&#x27;./dataset/CIFAR10&#x27;</span>, train=<span class="literal">False</span>, transform=torchvision.transforms.ToTensor(),</span><br><span class="line">                                       download=<span class="literal">True</span>)</span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyModule</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyModule, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_channels=<span class="number">3</span>, out_channels=<span class="number">6</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, <span class="built_in">input</span></span>):</span><br><span class="line">        output = self.conv1(<span class="built_in">input</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line">module = MyModule()</span><br><span class="line">writer = SummaryWriter(<span class="string">&#x27;logs&#x27;</span>)</span><br><span class="line">step = <span class="number">1</span></span><br><span class="line"><span class="comment"># print(output)</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, target = data</span><br><span class="line">    output = module(imgs)</span><br><span class="line">    <span class="comment"># print(output.shape)</span></span><br><span class="line">    writer.add_images(<span class="string">&#x27;input&#x27;</span>, imgs, step)</span><br><span class="line">    output = torch.reshape(output, (-<span class="number">1</span>, <span class="number">3</span>, <span class="number">30</span>, <span class="number">30</span>))</span><br><span class="line">    writer.add_images(<span class="string">&#x27;output&#x27;</span>, output, step)</span><br><span class="line">    step += <span class="number">1</span></span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure><p><strong>最大池化：减少参数的同时保留特征</strong></p><p>重要参数：ceil_model、kernel_size</p><p>torch.tensor 生成矩阵的时候，记得加上dtype=torch.float32这个参数，不然就被识别成Long型无法处理。</p><p><strong>模型的保存和提取：</strong></p><p>torch.save()</p><p>torch.load()</p><p><strong>模型的参数更新：</strong></p><p>先选择优化器，optimizer，然后optimizer.zero_grad()</p><p>然后求LOSS，对求完的LOSS进行backward()</p><p>然后optimizer.step()</p>]]></content>
      
      
      <categories>
          
          <category> Coding </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> Pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>labelme安装避坑指南</title>
      <link href="/posts/b8ce5541.html"/>
      <url>/posts/b8ce5541.html</url>
      
        <content type="html"><![CDATA[<h3 id="一、Anaconda安装">一、Anaconda安装</h3><ol><li><p>Anaconda<a href="https://www.anaconda.com/products/individual">下载</a>对应电脑版本的，windows10系统的直接点这个就可以下载</p><img src="https://imagebed-2jk.pages.dev/img/labelme安装避坑指南_1_1.png" width = "80%"></li><li><p>下载完成之后运行安装程序，按照如下步骤点击安装。（大概需要2.6G空间，安装路径默认就好）</p><p>依次点击：</p><figure class="highlight mathematica"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">Next</span><span class="operator">&gt;</span>  </span><br><span class="line"></span><br><span class="line"><span class="built_in">I</span> <span class="variable">Agree</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">All</span> <span class="variable">Users</span>  <span class="built_in">Next</span><span class="operator">&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">Next</span><span class="operator">&gt;</span></span><br><span class="line">这里出现两个勾选项，全部勾上  <span class="built_in">Install</span></span><br><span class="line"></span><br><span class="line">然后安装完成之后的<span class="variable">Finish</span>界面两个勾选项取消掉，然后单击<span class="variable">Finish</span></span><br></pre></td></tr></table></figure></li><li><p>到这里，Anaconda安装完成，开始写环境变量，在左下角的搜索栏搜索“环境变量”并打开</p><img src="https://imagebed-2jk.pages.dev/img/labelme安装避坑指南_1_2.png" width = "50%"></li></ol><img src="https://imagebed-2jk.pages.dev/img/labelme安装避坑指南_1_3.png" width = "45%"><p>点击“环境变量”，找到如下图所示的系统变量Path并双击打开</p><img src="https://imagebed-2jk.pages.dev/img/labelme安装避坑指南_1_4.png" width = "45%"><p>再点击新建，如下图内容输入下面的内容<code>C:\ProgramData\Anaconda3\Scripts</code>点击确定，这里就配置好了</p><img src="https://imagebed-2jk.pages.dev/img/labelme安装避坑指南_1_5.png" width = "45%"><h3 id="二、创建虚拟环境并安装Labelme">二、创建虚拟环境并安装Labelme</h3><ol><li>进入命令行，输入conda检查Anaconda是否安装成功，出现如下界面表示正确，进行下一步，否则重新安装Anaconda。</li></ol><img src='https://imagebed-2jk.pages.dev/img/labelme安装避坑指南_2_1.png' width='80%'><ol start="2"><li><p>输入<code>conda create -n labelme python=3.6</code>并回车，这里提示[y/n]，输入y然后回车。</p></li><li><p>依次输入如下命令，完成labelme安装：</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">conda activate labelme</span><br><span class="line"></span><br><span class="line">pip <span class="keyword">install</span> pyqt5</span><br><span class="line"></span><br><span class="line">pip <span class="keyword">install</span> labelme</span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li><p>至此，Labelme安装完成，如需使用按照如下步骤：</p><p>（1）Win+R 输入cmd打开命令行窗口；</p><p>（2）输入<code>conda activate labelme</code>；</p><p>（3）输入<code>labelme</code>；</p></li></ol><h3 id="三、json转换数据集">三、json转换数据集</h3><p>以上步骤能完成Labelme的安装和使用，如需转换json文件为数据集，还需要注意避如下坑：</p><ol><li><p>首先，labelme标准安装版本一次只能转换一个json文件，如果需要批量转换，需要参考其他操作。</p></li><li><p>转换的操作如下：</p><p>（1）进入到<code>.\Anaconda3\envs\labelme\Lib\site-packages\labelme\cli</code>目录下;</p><p>（2）在该目录进入cmd命令行；</p><p>（3）输入<code>labelme_json_to_dataset.exe &lt;你的json文件路径&gt;</code>；</p></li><li><p>某些版本的labelme可能会导致转换之后的文件缺少info.yaml文件，需要做如下修改：</p><p>（1）进入如下目录：<code>.\Anaconda3\envs\py36_labelme\Lib\site-packages\labelme\cli</code>，找到名为json_to_dataset.py文件；</p><p>（2）在json_to_dataset.py中添加如下内容：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">    PIL.Image.fromarray(img).save(osp.join(out_dir, <span class="string">&#x27;img.png&#x27;</span>))</span><br><span class="line">    utils.lblsave(osp.join(out_dir, <span class="string">&#x27;label.png&#x27;</span>), lbl)</span><br><span class="line">    PIL.Image.fromarray(lbl_viz).save(osp.join(out_dir, <span class="string">&#x27;label_viz.png&#x27;</span>))</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(osp.join(out_dir, <span class="string">&#x27;label_names.txt&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> lbl_name <span class="keyword">in</span> label_names:</span><br><span class="line">            f.write(lbl_name + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line"><span class="comment"># 缺少的部分，需要添加</span></span><br><span class="line">    logger.warning(<span class="string">&#x27;info.yaml is being replaced by label_names.txt&#x27;</span>)</span><br><span class="line">    info = <span class="built_in">dict</span>(label_names=label_names)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(osp.join(out_dir, <span class="string">&#x27;info.yaml&#x27;</span>), <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        yaml.safe_dump(info, f, default_flow_style=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">logger.info(<span class="string">&#x27;Saved to: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(out_dir))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>（3）重新按照2中操作转换json文件即可；</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Environments </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python packages </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>openslide安装避坑指南</title>
      <link href="/posts/8f5ba4dd.html"/>
      <url>/posts/8f5ba4dd.html</url>
      
        <content type="html"><![CDATA[<h3 id="一、Openslide简介">一、Openslide简介</h3><ol><li><p>Openslide是一个python的包。</p></li><li><p>Openslide包是用来打开并处理医学图片的，类似.ndpi或者.mrxs这种全扫描片，目前都是通过openslide来进行处理的。</p></li></ol><h3 id="二、openslide安装">二、openslide安装</h3><ol><li><p>不能直接pip，要先下载<a href="https://openslide.org/download/">二进制文件</a>；</p><img src='https://imagebed-2jk.pages.dev/img/openslide安装避坑指南_1_1.jpg' width='70%'></li><li><p>把下载好的二进制文件解压到自己喜欢的目录（如果是Anaconda环境，需要解压到Anaconda的Library目录下）；</p></li><li><p>然后配置环境变量，在系统变量Path里新建如下两个路径；</p><img src='https://imagebed-2jk.pages.dev/img/openslide安装避坑指南_1_2.jpg' width='40%'><p>比如解压在D盘的根目录下，则新建的路径如下：</p><figure class="highlight apache"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">D</span>:\openslide-win64-<span class="number">20171122</span>\openslide-win64-<span class="number">20171122</span>\bin</span><br><span class="line"><span class="attribute">D</span>:\openslide-win64-<span class="number">20171122</span>\openslide-win64-<span class="number">20171122</span>\lib</span><br></pre></td></tr></table></figure></li><li><p>Pip安装Openslide包，直接使用以下指令</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install openslide-python</span><br></pre></td></tr></table></figure></li><li><p>这里我们需要到openslide的安装目录下，修改名为lowlevel.py的文件，添加如下代码：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.environ[<span class="string">&#x27;PATH&#x27;</span>] = <span class="string">&quot;#这里为openslide安装目录的bin目录&quot;</span> + <span class="string">&quot;;&quot;</span> + os.environ[<span class="string">&#x27;PATH&#x27;</span>]</span><br></pre></td></tr></table></figure><p>注意：这里的openslide安装目录并不是上文第二步中提到的解压目录，而是第四步中pip的安装目录。非Anaconda环境的目录在<code>C:\Users\username\AppData\Roaming\Python\Python37\site-packages\openslide</code> 下；Anaconda环境的目录在Anaconda安装目录的<code>.\envs\envirname\Lib\site-packages\openslide</code>。</p><p>更新：这里如果用的是python3.8及以上的版本，以上代码需要更改，因为3.8以上版本的读取路径方式发生了修改。</p></li><li><p>至此，Openslide安装就完成了，import openslide成功！</p><p>备注：如果第六步报错，删掉lowlevel.py里的<code>from . import _convert</code>之后再次尝试import openslide便可成功。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Environments </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python packages </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Camouflaged Object Detection 阅读笔记</title>
      <link href="/posts/6d212f87.html"/>
      <url>/posts/6d212f87.html</url>
      
        <content type="html"><![CDATA[<h3 id="0-Summary">0. Summary</h3><p>  伪装目标检测（Camouflaged object detection，COD）旨在对几乎完全融入周围环境的目标进行识别。该问题的主要难点在于：待检测目标与周围背景具有极高的内在相似性。本文为了解决这样一个问题，提出了名为COD10K的伪装目标检测数据集，并针对该数据集，提出了 Search Identification Network (SINet) 网络模型。该网络模型在包括COD10K以及其他测试数据集上与其他最新的目标检测模型做对比实验，实验结果优于其他所有模型。</p><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Fan_Camouflaged_Object_Detection_CVPR_2020_paper.html">论文传送门</a>，<a href="https://github.com/DengPingFan/SINet">源代码传送门</a>。</p><h3 id="1-Introduction">1. Introduction</h3><p>  伪装目标检测大概需要做什么工作或者实现什么效果呢？如图1.1，第一行数据来自于COD10K数据集，第二行就是需要实现的检测效果。可以很清楚的看出，文章做的工作其实就是对于图片中一些边界与背景具有高相似度的物体进行识别或者分割。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_1_1.jpg" alt="图1.1 部分COK10K数据集中数据（row 1）及模型识别效果（row 2)"></p><div align = "center">图1.1 部分COK10K数据集中数据（row 1）及模型识别效果（row 2）</div> <p>  本文主要贡献：</p><ol><li><p>提出了COD10K数据集，专门用于COD。该数据集包括了10K张图片，目标类别包括78种水生、飞行、两栖动物、地面动物。其中每张图片的伪装目标都分层次地进行了标注，标注内容包括类别（category）、边界框（bounding-box）、对象等级（object-level）和实例等级（instance-level）。</p></li><li><p>通过对比COD10K和其他两个现有的数据集，本文提供了12个SOTA（state-of-the-art）基线（baseline）的严格评估，使文章中的研究成为最大的COD研究。</p></li><li><p>文章提出了SINet（Search and Identification Net）伪装目标检测框架，整个框架的训练时间仅为1小时，但是却在所有现有COD数据集上达到了SOTA的性能，在深度学习领域开创了首个完整的伪装目标检测的benchmark。</p></li></ol><h3 id="2-Related-Work">2. Related Work</h3><p>  通过查阅文献，可以将目标大致分类为三种类别：<strong>一般目标</strong>、<strong>突出目标</strong>、<strong>伪装目标</strong>。</p><ul><li><p>GOD（Generic Object Detection）：</p><p>最流行计算机视觉的思想之一中认为一般目标可能是突出的，也可能是伪装的；伪装目标可以被看做是复杂情况的一般目标。典型的GOD任务包括了语义分割和全景分割任务。</p></li><li><p>SOD（Salient Object Detection）:</p><p>SOD任务旨在识别图片中最突出的（most attention-grabbing）目标，并且分割出它的像素级轮廓。尽管“突出”和“伪装”两个词语的意思是相对立的，但是包含突出目标的图片可以作为伪装目标检测数据集的负样本。</p></li><li><p>COD（Camouflaged Object Detection）：</p><p>  <u>伪装目标分类</u>：伪装图片可以被粗略的分为两类——天然伪装和人工伪装。天然伪装是指动物为了避免被其天敌猎杀从而实现的一种生存技能；人工伪装通常发生在产品的制造过程（缺陷）或者用于游戏、艺术的隐藏信息。</p><p>  <u>评估矩阵</u>：由于MAE（Mean absolute error）已经被广泛应用于SOD任务中，因此文本也尝试使用MAE作为评估预测图C和真实结果G之间像素级准确率的度量。虽然MAE能够评估误差和错误的数量，但是MAE不能确定错误发生的地方。所以将采用基于人类视觉感知的E-measure来评估整体结果和伪装目标检测的定位精度。同时，由于伪装目标通常有不同的形状，因此还需要一个用于评估结构相似度的度量，本文考虑了S-measure和F-measure作为该任务的度量模型。</p></li></ul><h3 id="3-Proposed-Dataset">3. Proposed Dataset</h3><p>  这是一个原文作者提出的一个有10,000张伪装目标图片的数据集COD10K，其中的数据大部分来自Flicker（笔者注：笔者这里怀疑是作者的笔误，并没有名为Flicker的图片网站，应该是<a href="https://www.j-h-k.com/photography-flickr.html">Flickr</a>。），其余的图片（大概200张）来自于其他网站。同时为了避免选择偏差（selection bias），该数据集还搜集了3,000张突出目标图片，1,934张非伪装目标图片。关于数据集COD10K的具体细节如图3.1所示，（a）图表示了分类学系统及其直方图分布，（b）图代表了数据集中的图像分辨率分布，（c）图表示了词云分布，即标签中所用词语的地理分布，（d）图表示了不同类别的Object/Instance的数量分布，（e）图展示了一些子类。</p><p>  <strong>关于Object和Instance的区分</strong>：作者强调现有的COD数据集主要聚焦在object-level上，然而能够一个对象解析为它的实例对于计算机视觉研究者们是否能够编辑或者理解一个场景来说是至关重要的。因此，作者进一步在实例级（instance-level）上标注了数据集。</p><p>  标注完成之后，从COD10K数据集中的每个子类中总共随机挑选6000张图片作为训练集，4000张图片作为测试集。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_3_1.jpg" alt="图3.1 来自COD10K数据集的统计和伪装类别示例"></p><div align = "center">图3.1 来自COD10K数据集的统计和伪装类别示例</div><h3 id="4-Proposed-Framework">4. Proposed Framework</h3><h4 id="4-1-Overview">4.1 Overview</h4><p>  文中提出的SINet框架主要由**Search Module（SM）<strong>和</strong>Identification Module（IM）**两个模块组成，其中SM模块负责寻找伪装目标，IM模块主要负责对伪装目标的位置进行精确定位。SINet的框架模型如图4.1所示，其中的具体模块的解释将在一下两个部分介绍。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_4_1.jpg" alt="图4.1 SINet网络结构"></p><div align = "center">图4.1 SINet网络结构</div><h4 id="4-2-Search-Module（SM）">4.2 Search Module（SM）</h4><h5 id="4-2-1-Bconv（Conv-BN-ReLu）">4.2.1 Bconv（Conv+BN+ReLu）</h5><p>  Bconv是由Convolution、Batch Normalization（BN）和ReLu激活函数共同组成的一个卷积层，其中BN通过将数据标准化，能够加速权重参数的收敛。ReLu作为激活函数,其形式如图4.2所示。从图中不难看出，ReLu函数将所有的负值变为0，而所有的正值都不变，这种操作叫做<strong>单侧抑制</strong>。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_4_2.jpg" alt="图4.2 Relu激活函数"></p><div align = "center">图4.2 ReLu激活函数</div><h5 id="4-2-2-Receptive-Field（RF）">4.2.2 Receptive Field（RF）</h5><p>  感受野（RF）组件包括了五个分支，如图4.3。在每个分支的第一个卷积层均有32个尺寸为1*1的卷积核，目的是为了将原图在不改变分辨率的前提下将图像通道减少到32个。并且，前三个分支后紧连着两个卷积层，紧接着前三个分支后都紧跟着不同尺寸的空洞卷积层，然后将卷积后的结果与第四个分支的结果进行连接操作之后使用一个Bconv层进行卷积，最后将第五个分支的结果与卷积之后的结果加起来作为RF模块的输出。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_4_3.jpg" alt="图4.3 RF结构图"></p><div align = "center">图4.3 Receptive Field结构图</div><p>  SM模块做的工作主要是通过使用不同层次的特征和RF模块，初步定位图像中伪装目标的位置。也就是论文原文里提到的猎人定位猎物位置的那一步。接下来就要使用IM模块来对伪装目标的轮廓进行识别，即对目标进行精确定位。</p><h4 id="4-3-Identification-Module（IM）">4.3 Identification Module（IM）</h4><p>  SM（Identification Module）模块的作用对应于文中提到的猎人对目标进行精准定位的部分。在这个部分中提到了两个组件，一个是PDC（Partial Decoder Component），通过阅读论文，笔者认为PDC模块是将SM模块的功能组件进行了集成之后的一个功能模块。首先PDC模块是将RF模块的一些输出通过PDC模块得到一个输出Ccsm。另外一个模块是基于注意力机制提出的SA（Search Attention），SA模块的输入为X2层的特征和PDC组件的输出经过Sigmoid激活函数的结果，联合这两个输入，通过注意力机制消除无关特征的干扰，然后通过RF组件扩大感受野，最终得到另一个输出Ccim。</p><p>  得到上述两个输出Ccsm和Ccim之后，使用交叉熵损失函数对这两个输出结果进行训练，得到最终的训练模型。</p><h3 id="5-Benchmark-Experiment">5. Benchmark Experiment</h3><h4 id="5-1-Experimental-Settings">5.1 Experimental Settings</h4><p>  本文中的实验部分训练集采用了CAMO、COD10K和CAMO+COD10K+EXTRA，其中CAMO和COD10K使用数据集中的默认训练集；测试集使用了CHAMELEON数据集和CAMO、COD10K的测试集。关于Baseline方面，由于没有公开可用的基于深度网络的COD模型，所以文中基于以下三个标准选取了12个深度学习baselines：（1）经典模型，（2）最近发表的模型，（3）在某个特定领域达到SOTA表现的模型，例如GOD和SOD。这些选取到的模型都用建议的参数设置进行了训练。</p><h4 id="5-2-Results-and-Data-Analysis">5.2 Results and Data Analysis</h4><p>  SINet以及文中选取的12个模型在CHAMELEON、CAMO-Test、COD10K-Test上的测试结果如图5.1所示，可以很明显的看出，在三个测试数据集上，SINet的表现均优于其他模型。特别地，GOD模型（如FPN）的表现比SOD模型（如CPD、EGNet）要更差一些，表明SOD框架可能更加适用于COD任务。但是，相比于SOD和GOD模型，SINet显著降低了训练时长（比如：SINet训练一小时，而EGNet则需要训练48小时）且还能在所有数据集上达到SOTA的表现，这说明SINet可能是COD问题的一个有希望的解决方法。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_5_1.jpg" alt="图5.1 定性结果"></p><div align = "center">图5.1 不同模型在不同数据集上的定性结果</div><h4 id="5-3-Cross-dataset-Generalization">5.3 Cross-dataset Generalization</h4><p>  由于数据集的普遍性和难度在训练或者评估不同算法的时候起着至关重要的作用，因此文中实验部分还采用了Cross-dataset Generalization（跨数据集泛化），其思路是：在一个数据集上训练模型，在其他数据集上进行测试。具体操作为：从CAMO和COD10K两个数据集中随机选取800张图片作为训练集，200张图片作为测试集。为了公平起见，作者在每个数据集上训练SINet直到loss稳定。结果如图5.2所示，其中“Self”是指训练和测试在同一个数据集上，“Mean Others”表示除了自己以外其他的平均分（这里用到的训练集和测试集的设置与图5.1中的设置不同，所以不具有性能可比性）。图上结果表明，COD10K的确是最复杂的，因为该数据集有大量的具有挑战性的伪装目标检测图。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_5_2.jpg" alt="图5.2 跨数据集泛化"></p><div align = "center">图5.2 跨数据集泛化的S-measure</div><h4 id="5-4-Qualitative-Analysis">5.4 Qualitative Analysis</h4><p>如图5.3，为SINet与另外两个baseline算法的定性比较结果。可以看到的是，PFANet虽然能定位伪装目标的位置，但是输出不够精确。EGNet由于使用了边界特征，能够比PFANet更好的定位边界，但是仍然会丢失一些细小的细节，比如第一行中的鱼。对于不确定的边界，遮挡和小物体这些具有挑战性的例子，SINet能通过细节推测出伪装目标的位置，进一步说明了模型的健壮性。</p><p><img src="https://imagebed-2jk.pages.dev/img/Camouflaged-object-detection_5_2.jpg" alt="图5.3 定性分析"></p><div align="center">图5.3 SINet和两个top-performing的baselines在COD10K上的定性分析</div><h3 id="6-Potential-Applications">6. Potential Applications</h3><p>COD潜在的应用场景文中提到了：医学图像分割、野外搜救或者嵌入图搜索引擎。</p><h3 id="7-Conclusion">7. Conclusion</h3><p>本文提出了极具贡献意义的数据集COK10K和COD领域的新baseline——SINet框架。在今后的工作中，作者计划扩展COD10K数据集，提供各种形式的输入，例如RGB-D伪装目标检测(类似于RGB-D显著目标检测等等）。新技术如弱监督学习、Zero-shot Learning、VAE和多尺度主干网络都会探索。</p>]]></content>
      
      
      <categories>
          
          <category> Papers </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/posts/4a17b156.html"/>
      <url>/posts/4a17b156.html</url>
      
        <content type="html"><![CDATA[<h3 id="1-博客内容">1.博客内容</h3><p>也没有什么具体的原因，大概就是脑子一热，想着做个自己的博客。</p><p>本博客主要以深度学习中的计算机视觉方向为主（本人读博方向，学习中），不定期更新。</p><h3 id="2-特别说明">2.特别说明</h3><p>要特别说明的是本博客的框架是hexo，采用评论系统 disqus（需要科学上网），点击头像跳转github也需要科学上网。</p><p>本博客还有打赏功能哦！~</p>]]></content>
      
      
      <categories>
          
          <category> Life </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
